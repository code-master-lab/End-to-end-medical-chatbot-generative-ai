{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98bdb5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b22df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\rautl\\\\OneDrive\\\\Desktop\\\\Generative-ai\\\\End-to-end-medical-chatbot-generative-ai\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b53eb4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a8654d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\rautl\\\\OneDrive\\\\Desktop\\\\Generative-ai\\\\End-to-end-medical-chatbot-generative-ai\\\\research'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c040bc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: 'End-to-end-medical-chatbot-generative-ai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnd-to-end-medical-chatbot-generative-ai\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'End-to-end-medical-chatbot-generative-ai'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"End-to-end-medical-chatbot-generative-ai\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92bd1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\rautl\\\\OneDrive\\\\Desktop\\\\Generative-ai\\\\End-to-end-medical-chatbot-generative-ai'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d11e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3325af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Extract Data From the PDF File\n",
    "# def load_pdf_file(data):\n",
    "#     loader= DirectoryLoader(data,\n",
    "#                             glob=\"*.pdf\",\n",
    "#                             loader_cls=PyPDFLoader)\n",
    "\n",
    "#     documents=loader.load()S\n",
    "\n",
    "#     return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ea5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data = load_pdf_file(data=\"../Data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c8262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Text Chunks\n",
    "# def text_split(extracted_data):\n",
    "#     text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "#     text_chunks=text_splitter.split_documents(extracted_data)\n",
    "#     return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022aabdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks=text_split(extracted_data)\n",
    "# print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  text_chunksS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d4a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# IMPORTANT NOTE ABOUT `text_chunks` AND THE DATA INGESTION FLOW\n",
    "# -------------------------------------------------------------\n",
    "#\n",
    "# When we run a RAG (Retrieval-Augmented Generation) pipeline,\n",
    "# the *first* step usually looks like this:\n",
    "#\n",
    "#    1. Load PDF files\n",
    "#    2. Split PDF content into text chunks  → `text_chunks`\n",
    "#    3. Convert each chunk into embeddings\n",
    "#    4. Upload embeddings into Pinecone (or any vector database)\n",
    "#\n",
    "# HOWEVER — in this notebook `text_chunks` is EMPTY:\n",
    "#\n",
    "#    print(text_chunks)  →  []\n",
    "#    print(len(text_chunks))  →  0\n",
    "#\n",
    "# Why does this happen?\n",
    "# ---------------------\n",
    "# Because the local PDF file is *corrupted* and contains no\n",
    "# readable text. As a result:\n",
    "#\n",
    "#    extracted_data = []        # no documents loaded\n",
    "#    text_chunks = []           # nothing to split\n",
    "#\n",
    "# And therefore, we CANNOT run:\n",
    "#\n",
    "#    Pinecone.from_documents(text_chunks, ...)\n",
    "#\n",
    "# Without any text chunks, there is nothing to embed.\n",
    "#\n",
    "# -------------------------------------------------------------\n",
    "# SO HOW DOES OUR PROJECT STILL WORK?\n",
    "# -------------------------------------------------------------\n",
    "# Even though the local PDF is corrupted NOW, the embeddings\n",
    "# WERE ALREADY created earlier and uploaded into Pinecone.\n",
    "#\n",
    "# Pinecone permanently stores all vectors on the cloud.\n",
    "# This means:\n",
    "#\n",
    "#    ✔ Our vector database already has 6,859 embeddings\n",
    "#    ✔ We do NOT need to recreate chunks\n",
    "#    ✔ We do NOT need to rerun embedding creation\n",
    "#    ✔ We only need to *load* the existing Pinecone index\n",
    "#\n",
    "# Therefore, instead of:\n",
    "#\n",
    "#    Pinecone.from_documents(...)\n",
    "#\n",
    "# we correctly use:\n",
    "#\n",
    "#    PineconeVectorStore.from_existing_index(...)\n",
    "#\n",
    "# This loads previously-stored embeddings from Pinecone\n",
    "# WITHOUT needing local `text_chunks` or a PDF.\n",
    "#\n",
    "# -------------------------------------------------------------\n",
    "# WHEN SHOULD YOU REBUILD THE VECTOR DATABASE?\n",
    "# -------------------------------------------------------------\n",
    "# Only when:\n",
    "#    - You add a new PDF\n",
    "#    - You update the existing document\n",
    "#    - You fix the corrupted PDF\n",
    "#\n",
    "# In that case, follow the FULL pipeline again:\n",
    "#\n",
    "#    load_pdf → split_text → create_embeddings → upload_to_pinecone\n",
    "#\n",
    "# -------------------------------------------------------------\n",
    "# SUMMARY FOR FUTURE DEVELOPERS:\n",
    "# -------------------------------------------------------------\n",
    "# • If `text_chunks` is empty, you CANNOT generate new embeddings.\n",
    "# • But your project will STILL WORK if embeddings ALREADY exist\n",
    "#   inside Pinecone.\n",
    "# • Always use `from_existing_index` when loading an existing\n",
    "#   vector database.\n",
    "# • Only use `from_documents` when building a brand new index.\n",
    "#\n",
    "# This is a crucial part of maintaining a clean and scalable\n",
    "# RAG architecture.\n",
    "# -------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a5283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hugging Face Embeddings \n",
    "\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec61672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "018da6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rautl\\AppData\\Local\\Temp\\ipykernel_22156\\177019967.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\rautl\\miniconda3\\envs\\medibot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2b61c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7496ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a683150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be8551d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
    "GROQ_API_KEY=os.environ.get('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "502cefce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# ❌ OLD / WRONG CODE (before):\n",
    "# -------------------------------------------------------------\n",
    "# from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "# from pinecone import ServerlessSpec\n",
    "# pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "# \n",
    "# ❗ Problem:\n",
    "#   - This was using Pinecone v2 / GRPC client.\n",
    "#   - Your index was created in Pinecone v3 (Serverless).\n",
    "#   - API key + client version mismatch → 401 Unauthorized errors.\n",
    "#\n",
    "# -------------------------------------------------------------\n",
    "# ✔ NEW / CORRECT CODE (after):\n",
    "# -------------------------------------------------------------\n",
    "# Use the official Pinecone v3 client (no GRPC, no ServerlessSpec)\n",
    "# This matches the version installed AND the index shown in dashboard.\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "from pinecone import Pinecone       # Pinecone v3 SDK\n",
    "import os\n",
    "from dotenv import load_dotenv      # Load keys stored in .env file\n",
    "\n",
    "# Load environment variables (API keys, etc.)\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch Pinecone API key from .env\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Create Pinecone client using v3 SDK\n",
    "# ✔ This is the correct method for Pinecone Serverless indexes\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Name of your existing Pinecone index (must match dashboard exactly)\n",
    "index_name = \"medicalbot\"\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# NOTE:\n",
    "# We are NOT creating a new index here because:\n",
    "#   ✔ Your index already exists\n",
    "#   ✔ It already contains 6859 embeddings\n",
    "#   ✔ Re-creating it would overwrite or delete stored vectors\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# ❌ OLD CODE (Index Creation) — Do NOT run again:\n",
    "# pc.create_index(\n",
    "#     name=index_name,\n",
    "#     dimension=384,\n",
    "#     metric=\"cosine\",\n",
    "#     spec=ServerlessSpec(\n",
    "#         cloud=\"aws\",\n",
    "#         region=\"us-east-1\"\n",
    "#     )\n",
    "# )\n",
    "# -------------------------------------------------------------\n",
    "# END OF CONFIGURATION BLOCK\n",
    "# -------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bb45981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c3886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_pinecone import Pinecone\n",
    "\n",
    "# docsearch = Pinecone.from_documents(\n",
    "#     documents=text_chunks,\n",
    "#     embedding=embeddings,\n",
    "#     index_name=index_name\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "840d3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index \n",
    "\n",
    "from langchain_pinecone import Pinecone\n",
    "\n",
    "docsearch = Pinecone.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "# This step reads embeddings already stored inside Pinecone,\n",
    "# without needing your PDF or text_chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ae23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_pinecone import Pinecone as PineconeVectorStore\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565eb1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = docsearch.similarity_search(\"What is this document about?\", k=3)\n",
    "# docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db94a082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='09a78576-c162-4376-8fc6-b75112c7fa62', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 3.0, 'page_label': '4', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='er will be corrected in future editions.\\nThis book is printed on recycled paper that meets Environmental Pro-\\ntection Agency standards.\\nThe paper used in this publication meets the minimum requirements of\\nAmerican National Standard for Information Sciences-Permanence\\nPaper for Printed Library Materials, ANSI Z39.48-1984.\\nThis publication is a creative work fully protected by all applicable\\ncopyright laws, as well as by misappropriation, trade secret, unfair com-'),\n",
       " Document(id='3226a8d1-f4bc-4ce4-a8e7-b0408e254d05', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 3.0, 'page_label': '4', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='er will be corrected in future editions.\\nThis book is printed on recycled paper that meets Environmental Pro-\\ntection Agency standards.\\nThe paper used in this publication meets the minimum requirements of\\nAmerican National Standard for Information Sciences-Permanence\\nPaper for Printed Library Materials, ANSI Z39.48-1984.\\nThis publication is a creative work fully protected by all applicable\\ncopyright laws, as well as by misappropriation, trade secret, unfair com-'),\n",
       " Document(id='ace72eba-e74b-4e74-9f2d-263ae5b9e846', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 4.0, 'page_label': '5', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='Introduction.................................................... ix\\nAdvisory Board.............................................. xi\\nContributors ................................................. xiii\\nEntries\\nVolume 1: A-B.............................................. 1\\nVolume 2: C-F.......................................... 625\\nVolume 3: G-M....................................... 1375\\nVolume 4: N-S........................................ 2307')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is optional to chequek That everything is work fine it means docsearch fetching the data from the knowledge base pinecone\n",
    "query = \"What is this document about?\"\n",
    "results = docsearch.similarity_search(query, k=3)\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd71bd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "514a3228",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is Acne?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7f396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='cae53c34-60cf-47b6-9285-8455400cc6f8', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 39.0, 'page_label': '40', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='29da7ec7-f39d-476a-9ff7-882817caefc4', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 39.0, 'page_label': '40', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='42f135c5-857b-4740-a712-6228dcc20493', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 38.0, 'page_label': '39', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 2 25\\nAcne\\nAcne vulgaris affecting a woman’s face. Acne is the general\\nname given to a skin disorder in which the sebaceous\\nglands become inflamed. (Photograph by Biophoto Associ-\\nates, Photo Researchers, Inc. Reproduced by permission.)\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 25')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d96f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.4,\n",
    "    max_tokens=500,\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11d34bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78818bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Acne is a skin disorder in which the sebaceous glands become inflamed. It is characterized by the occurrence of pimples, blackheads, and other skin lesions. Acne vulgaris is the most common form of acne.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Build the final LLM chain (same job as StuffDocumentsChain in old LangChain)\n",
    "# This takes the prompt → runs it through the LLM → converts output to clean text\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# 5. Full RAG pipeline (same job as RetrievalChain in old LangChain)\n",
    "# Step 1: Retrieve top 3 most relevant chunks from Pinecone based on user question\n",
    "# Step 2: Merge those chunks into one single context block\n",
    "# Step 3: Send {context + question} to the LLM and return its answer\n",
    "def rag_pipeline(query):\n",
    "    docs = docsearch.similarity_search(query, k=3)\n",
    "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    return chain.invoke({\"context\": context, \"input\": query})\n",
    "\n",
    "# 6. Test the chatbot by asking a question\n",
    "rag_pipeline(\"What is Acne?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca0b5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# GROQ LLM SANITY CHECK (WHY THIS BLOCK IS IMPORTANT)\n",
    "# -------------------------------------------------------------\n",
    "# Before using Groq inside your RAG pipeline, we MUST test:\n",
    "#   1. API key loading from .env file\n",
    "#   2. Authentication with the Groq server\n",
    "#   3. Whether the selected model exists and is available\n",
    "#   4. Whether the client can generate a basic response\n",
    "#\n",
    "# If this block works → your whole chatbot will work.\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (GROQ_API_KEY)\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Groq client using the API key from .env\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Send a simple test message to verify:\n",
    "#   ✔ Correct API key\n",
    "#   ✔ Correct model name (llama-3.1-8b-instant)\n",
    "#   ✔ Groq servers responding properly\n",
    "#   ✔ Client library works\n",
    "# -------------------------------------------------------------\n",
    "chat = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",         # Groq's fast + free model\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    ")\n",
    "\n",
    "# Print the model's response to confirm everything is working\n",
    "print(chat.choices[0].message.content)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# If you see a response (like \"How can I assist you today?\"):\n",
    "#   ✔ Your Groq LLM is fully functional.\n",
    "#   ✔ Safe to integrate with RAG pipeline.\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
